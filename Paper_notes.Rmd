---
title: "Paper Summary"
author: "Narad Mishra"
output: html_document 
---

## Watson & Crick, 1953
Watson, J. D., and F. H. Crick. *"Molecular Structure of Nucleic Acids; a Structure for Deoxyribose Nucleic Acid."* Nature 171, no. 4356 (April 25, 1953): 737-38   

  In this paper, Watson & Creek wish to suggest the structure of Deoxyribose Nucleic Acid (DNA), which they have refered to as a salt of Deoxyribose Nucleic Acid. A structure for DNA was previously proposed by Pauling and Corey, that pictured the DNA as having three inter-twined chains with the phosphates near the fibre axis, and the bases on the outside. This structure didn't have explanation on how the chains having all negatively charged stayed that close without being repelled. Their proposed structure of DNA consist of two helical chains each coiled around a fibre axis. The two chains are related by a dyad perpendicular to the fibre axis. Each chain consist of bases on te inside and the phosphates on the outside. In this structure, the sugars are being roughly perpendicular to the attached base. The bonding between the bases of a chain to the base of other chain makes the two chains to stick together. Purine base of a chain is hydrogen bonded with pyrimidine base of the other chain. This bonding are perpendicular to the fibre axis. This bonding is said to be in a way such that purine position 1 bonds to pyrimidine position 1, and also position 6 of purine bonds to position 6 of pyrimidine. Due to this specific base pairing, it is possible to come up with the sequence of a chain given the sequence of the other chain. Previously available x-ray data was insufficient to prove their proposed structure, so they wanted the work to be regarded as unproved until the structure is checked against more exact results. They finally specified that their proposed specific base pairing suggest for possible copying mechanism for the genetic material. The paper acknowlegdes the help and support they received from different individuals and papers.
  
---


## High density synthetic oligonucleotide arrays

  This paper written by Robert J. Lipshutz, et.al., is a discreption of DNA probe arrays developed by a company Affymetrix. They have used the simple principle of recognition and hybridization ability of DNA sequnce to develop a new experimental tools designed to collect and analyse vast amounts of genetic and cellular information. The high-density DNA probe arrays developed at Affymatrix used two techniques, photolithography and solid-phase DNA synthesis. This high density DNA probe arrays are used for gene expression monitoring, which then is used for genotypic analysis. The paper describes on how this Oligonucleotide arryas for gene expression monitoring are designed, how are they analyzed and new tools and softwares that are currently being developed by Affymetrix.

---

## Expression profiling; best practices for data generation and interpretation in clinical trials

  This papar talks about the importance of Microarrays for mRNA transcript expression level detection and analysis. These microarrays are being used increasinly in clinical trials. Due to regulations in clinical trials, microarry technology and the data generated must have clear set standards for its use. This paper tries to find the best practice for data generation and its interpretation, so it can be used in clinical trials. For this, groups like *Microarray Gene Expression Data* (MGED) society, *Minimum Information About A Microarray Experiment* (MIAME), and the MAGE-ML mark-up language are step closer to aquaring unity and standarizing the procedure on how to approach to the obtaining of and interpretation of genomic data. The paper tries to show the key differences between Affymetric arrays, spotted cDNA, and spotted oligonucleotide. The paper also describes how and where could there be possible variation in the data and recommends the best practice to experimental designs. 
  
---

## Points of Significance: Sources of Variation.

This article is written by Naomi **Altman and Martin Krzywinski,** and was published in "Natrue Methods" on 30th December 2014. This article describes different sources of variation in biological experiments, and how should they be dealt with. Amount of variability determines the ability to replicate (internal validity) and generalization to the entire population (external valildity). controlling the variablibity will make the experiment more replicable but it will negatively effect on making the outcome to more general population.So, the paper talks about balancing these two factor while controlling for the variablity. For biological experiments, one should try to reduce the variablity just enough to make the experiment/outcome both reproducible and be generalized to the wider population.The paper also talks about how these different sources of variabilities can be used to infer about the effective sample size.Paper describes different sources that contribute to the overall variation. One needs to determine the source where the variation is coming from to effectively take care of that variation.


---

## A Comparison of Normalization Methods for High Density Oligonucleotide Array Data Based on Variance and Bias.
 
This paper is written by B.M Boland at al.and was published by *Bioinformatics* journal volume 19 in 2003. In this paper, writer compares the different methods of normalizing microarray data. Normalization is a method of reducing data into more manageable and meaningful state. In the paper, they compare the performance of three complete data methods of normalization and then compare each of those method to the methods that uses baseline array instead of using complete array data. Three complete data methods compared were, *cyclic Loess*, *contrast based method*, and *quantile normalization*. Among these three methods, quantile normalization method was found to perform slightly better than other two methods. In terms of speed, quantile method was again found to perform better than other two methods that uses complete data. 
The two methods that are described in this paper that uses baseline array for normalization are *scaling method*, and *non-linear method*. Non-linear method was found to perform much closer to quantile method when 27-spike in array data was analyzed. Over all, quantile method of normalization performed well in most circumstances and was the recommended method to use for normalization of microarray data.


---

## Locally Weighted Regression: An Approach to Regression Analysis by Local Fitting.

This article was published by  *Journal of American Statistical Association* in september 1988, and was written by Willim s. Cleveland and Susan J. Devlin. In the paper, the writers describe about a method of estimating a regression surface through a multivariate smoothing procedure called *Loess*. The article shows how *Loess* can be used for various purpose, such as data exploration, diagnostic checking of parametric models, and provideing a non-parametric regression surface. *Loess* can not only be used in microarray data, but in variety of circumstances and data types. The paper describes few of the areas where *loess* is appropriate. As an example in the paper, *loess* method is used for measuring velocity of galaxies, to estimate concentration of ozone, both by locally weighted regression. It extensively describes the uses and advantages of using *loess* method for regression analysis, but it also highlights the restriction and few drawbacks of using *loess*, a locally fitted regression.


---

## Significance Analysis of Microarrays Applied to the Ionizing Radiation Response.

This article was published by *Proceedings of the National Academy of Science* volume 98, on april 24th 2001. The paper is written by Virginia Goss Tusher, Robert Tibshirani, and Gilbert chu. The paper describes the need of a method to perform analysis on microarray data to find the differentially expressed genes. The paper starts with how the existed methods works and their inability of being accurate. Here they talk about a method called as SAM, which works much better at predicting the differentially expressed genes correctly. With this method, each gene is assigned a score on the basis of its change in gene expression relative to the standard deviation of repeated measuremetns for that gene. Genes with higher or lower value than the threshold are considered potentially differentially expressed. SAM then calculates the false discovery rate (FDR) from those genes that are seen above and below the threshold values. The genes are then ranked by their values, so that the largest gets the first in the order. For each permutation of genes, relative differences are again calculated and the differences are then ranked once again. Expected relative difference for each gene is then plotted against the relative distance rank obtained for each gene to identify significant differentially expressed genes. They have used this method to analyse a biologically important problem, where transcriptional response of lymphoblastoid cells to ionizing radiation was analysed. It was found that SAM identifies much lower percentage of genes as potential genes that are differentially expressed. This corresponds to having much lower FDR (12%) compared to 60% and 84% of FDR when conventional method of analysis was used.


---

## Linear MOdels and Empirical Bayes Methods for Assessing Differential Expression in Microarray Experiments.

This article was published by *Statistical Applications in Genetics and Molecular Biology* in 2004, volume 3, issue 1, article 3. It is written by Gordon K. Smyth. This paper discuss the problem of identifying genes that are differentially expressed in designed microarray experiments, and how they propose to solve the problem. Smith, in this paper propose to develop the hierarchical model based on the model Lonnstedt and Speed (2002) introdueced, to counter the problem. A close form estimators for the hyperparameters are derived using marginal distributions of the observed statistics. Then the posterior odds statistic is reforulated in terms of a moderated t-statistic in which posterior residual standard deviations are used in place of ordinary standard deviations. This results in a shrinkage of the gene-wise residual sample variances towards a common value, resulting in more stable inference. Inference is done with the moderated t-statistic, which is equivalent to B-statistics, and doesn't depend on hyperparameters unlike the B-statistics. Simulation study showed that the moderated t-statistics had the lowest false discovery rate and highest receiver operating curves (ROCs), among other methods compared. 




---

## Controlling the False Discovery Rate: a Practical and Poweful Approach to Multiple Testing

This article was published in 1995, *Royal Statistical Society* and is written by Yoav Benjamini and Yosef Hochberg. This article describes the challenges we face in analyzing genomic data, where there are multiple test being carried simultaneously, and provides the methods to counter those challenges. It describes how the family wise error rate (FWER), and classical multiple-comparison procedures (MCPs) has drawbacks and are conservative in approach. The paper propose a method to counter the problem of multiple testing called False Discovery Rate (FDR). FDR is an expected proportion of errors among the rejected hypothesis. Paper also shows that FDR not only works better than the other methods but also implies weak control of FWER and is more powerful method. In the simulation study they carried to test for the power of the three methods, FDR method was crealy better than other two overall. 



---

## Statistical Significance for Genomewide Studies

This article was published in *Proceedings of the National Academy of Science* , august 5th 2005, volume 100, and the article is written by John D. Storey and Robert Tibshirani. This article propose a *q* value, which is an extension to the False Discovery Rate (FDR) a method discussed in earlier article. The paper further emphasize in the validity of the FDR method of multiple testing and introduce the *q* value which is a measure significance in terms of FDR, unlike *p* a measure of significance in terms of false positive rate. FDR as author mention is a first step to multiple testing but does not explain for the individual features and the *q* value that they propose provide this. Mathematically, *q* value for a test (*i*)is the mininum FDR that can be attained when calling that feature significant. 

---

## Ontologies in Biology: Design, Applications and Future Challenges

This article, written by Jonathan B.L.Bard and Seung Y. Rhee, was published by *Nature Reviews* on March 2004. This article explains how the advancement in technologies and science is providing immense amount of data to the public, but with the challenge of interpretation. As a solution to this interpretation and understanding problem, the article describes an importance of having a platform, called Ontology in Biology, which includes various subtypes with the platform. The article singles out those ontology type and explains how each of the ontology provides knowledge in microarray studies. Gene ontology, cell ontology, anatomical ontology, are the subtype of the broad biological ontology. Each ontology contains an ontology ID, which can be used to link between various platform for indepth knowledge using all kinds of ontologies. The Bio-ontologies have wide variety of use, from representation of knowledge in a computer-comprehensible way to annotaion and analysis of large-scale data with ontology IDs.


---

## Gene Set Enrichment Analysis: Performance Evaluation and Usage Guidelines

This article written by Jui-Hung Hung, et.al, was published by *Oxford University Press* in 2011. In this article, writers evaluate the approach to gene set enrichment analysis and explains each steps of the process. Further, the article also mentions that there is no gold standard to gene set enrichment, it is difficult to compare the performance of different gene set-level statistics. They performed a comparison between analytical background  and simulated background to check for the validity, and found that it was more accurate to use simulated background than analytical background. This backround information is used in estimating the significance in gene-set enrichment analysis. In gene-set enrichment analysis, the predictors are assumed to be orthogonal and is not the case most offen. For this, they proposed a Controlled Mutual Coverage (CMC) method, which takes care of the dependency between the predictors. when they tested their method with 132 experimental data sets, they found that the method substantially diminished the correlation. 
